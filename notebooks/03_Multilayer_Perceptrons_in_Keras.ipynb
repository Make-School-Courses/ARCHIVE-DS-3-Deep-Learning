{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=60% width = 60%></center>\n",
    "\n",
    "<center><h1>MultiLayer Perceptrons With Keras</h1></center>\n",
    "\n",
    "So far in this course, we've learned about Single Layers Perceptrons--the precursors to modern neural networks.  We've also learned about the activation functions used in neural networks, and the symbolic relationship they have with neurons in a brain. In this lesson, we'll put it all together and create a **_Multilayer Perceptron_** that uses multiple layers of neurons.  We'll use it to classify different types of clothing from the _Zalando Fashion Dataset_, a more complex dataset used for benchmarking and practice, in the same way that MNIST is used.  To build it, we'll use the [**_Keras_**](https://keras.io/) library--in industry-standard tool used by Deep Learning practicioners to quickly create Deep Neural Network prototypes.  Keras gives us all the power of Google's [**_TensorFlow_**](https://www.tensorflow.org/) framework, but with a clean, easy-to-use API that abstracts away all the boilerplate code needed to get TensorFlow working.\n",
    "\n",
    "<center><h2>Why We Need Multiple Layers in our Perceptrons</h2></center>\n",
    "\n",
    "Recall the structure of the Single-Layer Perceptron we built in unit 1:\n",
    "\n",
    "<center><img src='img/SLP_diagram.png' height=80% width=80%></center>\n",
    "\n",
    "This perceptron generally worked, but wasn't too effective at learning--with only one layer, it's just too simple to learn complex nonlinear functions.  Multilayer perceptrons solve this problem by stacking at least 3 layers of neurons together. At minimum, a Multilayer Perceptron must consist of:\n",
    "\n",
    "1. An Input Layer\n",
    "1. 1 or more **_'Hidden' Layers_**\n",
    "1. An Output Layer\n",
    "\n",
    "The hidden layer(s) are the 'secret sauce' that makes modern Deep Neural Networks so effective.  We'll discuss what they do, why they are considered 'hidden', and how they fit into modern Deep Neural Networks further along in this assignment.  Before we get to that, let's explore the history (and drama) that took us from Single-Layer Perceptrons to the powerful modern Neural Networks we use nowadays.\n",
    "\n",
    "\n",
    "<center><h3>Some History on Perceptrons (Nerd Drama Ahead!)</h3></center>\n",
    "\n",
    "We can stack them and have them work together to generate an overall prediction--but this comes with it's own problem.  Let's say we have the following neural network:\n",
    "\n",
    "<center><img src='img/mlp-network.png' height=80% width=80%></center>\n",
    "\n",
    "For the following analogy, it's helpful to think of each neuron as a person giving advice to the people in the following layer.  The people in the first layer take a look at the inputs, and (based on their weights from the previous layer, plus a bias), pass along their advice to each person in the next layer, who repeat this process until it gets to the person at the very end.  Based on the advice he received from the previous layer, who based their advice on the information they received from the previos layey (and so on and so on), the person makes a decision, which turns out to be wrong.  \n",
    "\n",
    "This leads us to a really tough question--**_how do we divide up the \"blame\" for this mistake?_** \n",
    "\n",
    "Let's think about all the ways this group of people (or neurons) could have gotten this wrong. What if everybody's weights and biases were wrong (meaning everyone gave bad advice)? Or, what if some people have perfected exactly how much weight they should give the inputs they receive, but the inputs themselves were bad? What if every neuron was perfect, but one was way off?  How would you even know which one? If we knew who to blame, and how much to blame them, we could shift their weights accordingly and let everyone \"learn\" from each mistake in the training data--but this wasn't possible, because we hadn't yet figured out how to tell who we should blame for a mistake (which weights we should adjust), and how much we should blame them (which would tell us how to adjust the weights accordingly).\n",
    "\n",
    "This was the central problem with neural networks, and the central insight behind the **_back propagation alogrithm_** (backprop for short) that powers the learning in modern neural networks. Although there were many pieces of the backprop algorithm invented as far back as 1960, no one really fully put all the pieces together until 1986, when Geoffrey Hinton, David Rummelhart, and Ronald J. Williams put all the pieces together and came up with the generalized backpropagation algorithm.  \n",
    "\n",
    "It should be noted that until backprop was invented, no one really realized just how powerful Neural Networks could be. Although people were very excited about Rosenblatt's invention of the SLP in 1957, this excitement died down in 1969 when Marvin Minsky published the book _Perceptrons_, which made strong claims that Perceptrons were only good for toy problems.  He showed proofs that a SLP can't solve the [XOR problem](https://www.quora.com/What-is-XOR-problem-in-neural-networks).  Once Hinton and team discovered backprop and compputers continued to get more and more powerful, that set the stage for the amazing Deep Learning explosion that we see today!\n",
    "\n",
    "\n",
    "<center><h2>Nuts and Bolts: Forward Prop and Back Prop</h2></center>\n",
    "\n",
    "When we train a neural network, what is actually happening? This can be summarized as two main steps:\n",
    "\n",
    "1. **_Forward Propagation_**\n",
    "2. **_Back Propagation_**\n",
    "\n",
    "Neural networks learn from training data in a similar way that we learn from doing practice problems and then checking to see if we got the answer correct.  First, we figure out an answer, and then we double check the answer to see if we got it right or wrong.  If we got it right, great--we understand how to solve that problem.  Don't change anything.  But when we get it wrong, we go back and figure out how we got it wrong, and shift our thinking.  This is generally what neural networks are doing. \n",
    "\n",
    "Before we build a working MLP in Keras, let's take a quick look at what happens during the forward prop and back prop steps for each data point in the training set. \n",
    "\n",
    "<center><h3>Forward Propagation</h3></center>\n",
    "\n",
    "Each feature for a row of data is passed in to a different neuron in the **_input layer_**.  If our dataset has 20 columns (often referred to as \"dimensions\"), then each column will be passed into a separate column meant just for it.  This data will then feed to every neuron in the first hidden layer.  Each neuron will then compute the weighted sum of the inputs it receives (which is just the [dot product](https://mathinsight.org/dot_product_examples) of the inputs and weights), plus a bias.  The neuron then passes this weighted sum through an activation function, which we learned in the last notebook lets our networks capture nonlinearity.  The number that the activation function spits out is the output of this neuron, which is also the input that will be passed on to all neurons in the next layer.  This pattern continues, passing information from **_hidden layer_** to hidden layer until the information reaches the **_output layer_**.  This output of the output layer is the prediction that the neural network has made for the data passed to it as input.  \n",
    "\n",
    "<center><h3>Back Propagation</h3></center>\n",
    "\n",
    "\n",
    "**_Loss:_**\n",
    "Since this is an instance of data from our training set, that means we have a label to check to see if the prediction made during the forward propagation step was correct.  To check our performance, we'll use a **_Loss Function_**.  You've already run across these when learning about things like regression--for example, the Sum of Squared Errors is a loss function.  The goal of the neural network during training is to shift the weights in a way that will minimize this loss function.  When it gets an a prediction correct, the loss will be 0.  This is a good thing, because if we got something correct, there's no reason to change the weights/biases around--this could do more harm than good!  However, when the network gets a prediction wrong, things get interesting.  Did it predict 0 when the label was 1, or 1 when the label was 0?  When learning from mistakes, how the network got something wrong is just as important as recognizing the mistake at all.  A clever loss function will tell us this. There are many clever loss functions in deep learning, but they are all generally some form of $y - \\hat{y}$, where $y$ is the actual label, and $\\hat{y}$ is the prediction made during the forward prop step.\n",
    "\n",
    "**_Gradient Descent_**\n",
    "\n",
    "If we output from our loss function, we can take the derivative of this loss function. If we have the derivative of this loss function, we can shift the weights up or down and see which direction decreases our loss.  If we have enough training data, and we shift the weights incrementally in a direction that decreases the loss every time we make a mistake, eventually we'll arrive at optimal values for each weight.  In layman's terms, our network will have \"learned\" the weights that will cause it to get the most predictions correct.  For an example, see the following diagram.\n",
    "\n",
    "<center><img src='img/gradient_descent.png' height=60% width=60%></center>\n",
    "\n",
    "**_Back Propagating the Error_**\n",
    "If our neuron only had one weight, gradient descent would be easy--we just shift the value for that weight in the correct direction every time until we have a gradient of 0. However, we have many different neurons in our network. What do we do? If you're familiar with calculus, you've probably heard of the **_Chain Rule_**. Our derivative of our loss function is decomposable.  After all, our loss comes from the output layer, but the input to that output layer was other functions, which also had inputs that were the outputs of other functions, all the way back to the beginning.  We can trace this chain backwards, taking the derivative at every step to propagate the error backwards and tell each neuron how to shift it's own weights, all the way back to the beginning! When we put all of this together, this is how we solve the blame assignment problem, which allows us to build very deep networks that can still assign \"blame\" appropriately and learn the correct weights with enough training data!\n",
    "\n",
    "\n",
    "(A quick note--you may be wondering where the initial values for the weights and biases for each neuron come from, before the first forward prop step on the first data point.  The simple answer is that they are just random values.  this is okay, because the network will \"learn\" the correct weights during the backprop step described below!)\n",
    "\n",
    "Phew--that was a lot to cover! Enough talk--let's use **_Keras_** to build our very own MLP for classifying pictures of articles of clothing from the **_Zalando Fashion MNIST dataset_**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following block of code to import everything we'll need for our MLP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras module explanations\n",
    "\n",
    "Let's take a look at the things we've imported directly from Keras, to get a better idea of what they are and how we'll use them.  \n",
    "```python\n",
    "from keras.models import Sequential\n",
    "```\n",
    "For most anything you'll be doing in Keras, you'll likely be using a Sequential model.  This is the actual model object that we'll create in our code, and then manipulate by adding layers.  \n",
    "```python\n",
    "from keras.layers import Dense\n",
    "```\n",
    "There are many different types of layers we can use inside our neural network.  The most simple one we can use a **_Dense_** layer.  You may have also heard this referred to as a **_Fully-Connected_** layer, meaning that each neuron in the layer receives the output from every single neuron in the previous layer, and passes its output to every neuron in the next layer.  The layers submodule also contains some other nifty tools, such as Dropout and BatchNormalization.  Although these aren't strictly necessary for building a Deep Neural Netowrk, they're very helpful for dealing with common problems such as overfitting! In this tutorial, we'll use them just to add some variety to our programming task--don't worry if you don't understand these concepts yet, as we'll cover them soon!\n",
    "\n",
    "```python\n",
    "from keras.datasets import fashion_mnist\n",
    "```\n",
    "\n",
    "This line just reads in an object that will import the fashion mnist dataset for us.  This is purely just to save us the time of having to import it all manually. \n",
    "\n",
    "Finally, we import two familiar libaries: `matplotlib` so we can visualize what some of our data looks like, and `numpy` for use in during our data preprocessing steps. \n",
    "\n",
    "**Run the cells below to impor the data and visualize an example from our data set. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for image: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11758da90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])\n",
    "print(\"label for image: {}\".format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Looking at the table below, we can see that this image is classified as an \"ankle boot\".  </center>\n",
    "\n",
    "\n",
    "| Label | Description |\n",
    "|-------|-------------|\n",
    "| 0     | T-shirt/top |\n",
    "| 1     | Trouser     |\n",
    "| 2     | Pullover    |\n",
    "| 3     | Dress       |\n",
    "| 4     | Coat        |\n",
    "| 5     | Sandal      |\n",
    "| 6     | Shirt       |\n",
    "| 7     | Sneaker     |\n",
    "| 8     | Bag         |\n",
    "| 9     | Ankle Boot  |\n",
    "\n",
    "<center>Next, we'll run through some common steps for preprocessing our data.</center>\n",
    "\n",
    "**Run the cell below to see the shape of X_train and X_test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (60000, 28, 28)\n",
      "Shape of X_test: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train: {}\".format(X_train.shape))\n",
    "print(\"Shape of X_test: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Data Preprocessing</h3></center>\n",
    "\n",
    "In order to prepare our data for use in our MLP, we'll need to do a couple things first:\n",
    "\n",
    "**1.  _Reshape each image into vectors of pixel values._**  \n",
    "    \n",
    "Currently, each image is a 28 x 28 matrix of pixel values.  We need to reshape this into a **_vector_** (a 1d array).  Numpy can help us with this. Right now, `X_train` is a **_Tensor_** of 60,000 28 x 28 matrices.  We need to turn this into a tensor of 60,000 784 x 1 vectors (28 x 28 is 784).  During this process,we'll also leverage the power of numpy to make sure that every single data point is the correct data type--inthis case, a `float32`.  \n",
    "\n",
    "To reshape X_train, use the following command from numpy:\n",
    "    \n",
    "```python\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "```\n",
    "\n",
    "<em>**You'll also need to reshape X\\_test--remember that X\\_test only contains 10000 rows, not 60000!**</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! This brings us to Step 2:\n",
    "\n",
    "**_ 2. Normalize the data._**  \n",
    "\n",
    "By normalizing our input data, this will allow the neural network to converge on the correct answers much more quickly.  In DS2, you learned that normalizing a value means subtracting the mean of the feature and then dividing by the standard deviation--however, since we're working with image data, there's a quick shortcut we can use.  Since each data point is a pixel value, this means that each data point can only have a value between 0 and 255. \n",
    "\n",
    "To normalize our image data, we'll just divide every point by 255! Example follows:\n",
    "\n",
    "```python\n",
    "X_train /= 255.\n",
    "```\n",
    "\n",
    "<em>**Divide X_train and X_test by 255.**</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our final preprocessing step:\n",
    "\n",
    "**_3. Convert labels to one-hot encoding_**\n",
    "\n",
    "As we saw when we looked at the example from our training set above, the label was just the integer `9`.  Since this is a multiclass classification problem with 10 classes, that means our output layer will contain 10 neurons, with the activation function for the output layer being softmax.  That means that we need to conver the labels to vectors of the same length to make computing our loss easier.  In each vector, every value will be a 0, except for the index that corresponds to the correct class for the label.  \n",
    "\n",
    "Current label format:\n",
    "`9`\n",
    "\n",
    "One-hot label format: \n",
    "`[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`\n",
    "\n",
    "Luckily, this is a common task, so _Keras_ includes utility functions that handle this for us. The code snippet below shows an example of how to use this:\n",
    "\n",
    "```python\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "```\n",
    "\n",
    "<em>**Use the function shown above to convert the labels contained in y_train and y_test to one-hot vectors.**</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Building our MultiLayer Perceptron</h2></center>\n",
    "\n",
    "Finally, we can build our first MLP.  Lets think about the architecture for this MLP:\n",
    "\n",
    "1. Our input layer needs to have 784 neurons--1 for each pixel value in our input vector.\n",
    "1. The MLP should contain at least 1 hidden layer.  Each layer should contain at least $\\sqrt{input\\ size}$ number of neurons.  It can be larger than this number, but if it's smaller than this number there's a solid chance it isn't big enough to adequately fit the data.  \n",
    "1. The output layer should have the same number of neurons as the total number of classes in the dataset.  \n",
    "\n",
    "We should also give some thought to which activation functions we'll use here.  Should we use **_sigmoid_**, **_tanh_**, or **_relu_**?  For our output layer, no decision needed--this is a multiclass classification problem, so we have to use **_softmax_** on the output layer.  \n",
    "\n",
    "**_That's alot of decisions to make!_** If you feel overwhelmed by trying to figure out exactly what the shape of your neural network should be, don't worry--that's normal.  These are **_hyperparameters_**, and there's no clear-cut rules for which ones you should choose.  This part takes a lot of experimentation. For now, let's start with a basic layout and modify as needed based on it's performance.  \n",
    "\n",
    "\n",
    "\n",
    "To create a neural network in Keras, we'll start by instantiating a `Sequential()` object, and then add each successive layer using the  object's `.add()` method. The code in the following cell creates a very basic MLP.  This network won't perform too well on the data, but don't worry about that just yet--we're only going to use it as a reference for how to write code with Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_model = Sequential()\n",
    "poor_model.add(Dense(25, activation='sigmoid', input_shape=(784,)))\n",
    "poor_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've created the model, we still have to have keras compile it.  Run the following code to compile and inspect the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                19625     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                260       \n",
      "=================================================================\n",
      "Total params: 19,885\n",
      "Trainable params: 19,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "poor_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "poor_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `.summary()` on a compiled model will give us a print out of each layer in the network, and tell us how many trainable parameters are contained in each layer, as well as in the model as a whole.  **_Trainable Parameters_** are any values that the model will learn through training--all the weights and biases that will start off as random values, each slowly moving towards its own optimal value with each mistake in training.  \n",
    "\n",
    "The network above is a pitifully small model for the problem at hand, and it still has **_19,885 trainable parameters!_**  The more neurons in a model, the more parameters it will have.  Larger models may be able to capture more complexity and do better on harder tasks, but with each additional parameter, the amount of time and training data needed to properly train the model goes up! This is the essential reason why we can't just throw bigger and bigger networks at problems and solve every problem in the world--it's also why Deep Learning took a decade or two to really start getting impressive results, even though back prop was figured out in 1986.  Computers then weren't powerful enough to train MLPs large enough to solve interesting problems--after a couple of iterations of [Moore's Law](https://www.investopedia.com/terms/m/mooreslaw.asp), computers became powerful enough that this was no longer a problem!\n",
    "\n",
    "**_Fitting the Model_**\n",
    "\n",
    "Like any machine learning algorithm, building the model isn't enough--we need to fit it to the training data.  Run the cell below to fit the model.  Pay attention to the output--what does the `loss` value do over time? What does the `accuracy` value do over time?  We also pass it our testing data to do validation at the end of each epoch--is there a disparity between the accuracy and validation accuracy? What might this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 1.8474 - acc: 0.4837 - val_loss: 1.5012 - val_acc: 0.6070\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 1.3027 - acc: 0.6652 - val_loss: 1.1567 - val_acc: 0.6883\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 1.0479 - acc: 0.7042 - val_loss: 0.9747 - val_acc: 0.7064\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.9039 - acc: 0.7273 - val_loss: 0.8667 - val_acc: 0.7271\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.8155 - acc: 0.7434 - val_loss: 0.7966 - val_acc: 0.7408\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.7552 - acc: 0.7562 - val_loss: 0.7456 - val_acc: 0.7532\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.7103 - acc: 0.7669 - val_loss: 0.7070 - val_acc: 0.7614\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.6748 - acc: 0.7761 - val_loss: 0.6759 - val_acc: 0.7708\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.6458 - acc: 0.7848 - val_loss: 0.6505 - val_acc: 0.7777\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.6216 - acc: 0.7913 - val_loss: 0.6291 - val_acc: 0.7855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b3950b8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poor_model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Fitting the Model: Vocabulary_**\n",
    "\n",
    "`batch_size=64`: The number of examples the model trains on before updating the weights.  A batch_size of 1 would mean that the model looks at every individual example, computes the loss, and updates the weights.  In the example above, the model works through training data in groups of 64, keeping a running total of how the parameters need to be updated, and then updates them at the end of every batch.  (Pro-tip: You'll usually get better runtime performance if your batch size is a power of 2.)\n",
    "\n",
    "`epoch=10`: The number of times the model trains on the entire training set. 1 full pass through the training data is 1 epoch.  In the code above, we went through the training data 10 full times. \n",
    "\n",
    "`verbose=1`: Tells Keras to give us more output rather than less during the training stage.  \n",
    "\n",
    "`validation_data=(X_test, y_test)`: This part is optional.  We pass it the validation data to have it check it's work against the test set at the end of every epoch.  During this step, it computes the forward prop step to make predictions on each data point in the test set, and computes the `loss` and `accuracy` based on the labels contained within `y_test`, but it does not update the weights.  This is strictly to tell us how we're doing. Validation metrics are more accurate than training metrics, because training metrics might be artifically high because of overfitting.  \n",
    "\n",
    "<center><h2>Building Your Own MLP</h2></center>\n",
    "\n",
    "Our example network wasn't very effective, but that's okay--the goal was only to give you some example code to help you get comfortable with the way things are done in Keras.  Next, you'll build your MLP with a more complex architecture and see how it improves on the poor example above!\n",
    "\n",
    "**_Build a network with the following architecture:_**\n",
    "\n",
    "1. An input layer of 784 neurons. \n",
    "1. A Dense layer of 128 neurons--sigmoid activation.\n",
    "1. A Dense layer of 64 neurons--sigmoid activation.\n",
    "1. A Dense layer of 10 neurons--softmax activation (this is our output layer)\n",
    "\n",
    "Use the example above to help you figure out the necessary syntax for building this model.  Make sure to add the layers in the order that you want them--the order that you add them will be the order that they appear in the final model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_model = Sequential()\n",
    "good_model.add(Dense(128, activation='sigmoid', input_shape=(784,)))\n",
    "good_model.add(Dense(64, activation='sigmoid'))\n",
    "good_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#keras.optimizers.RMSprop(lr=keras.o 0.00, rho=0.9, epsilon=None, decay=0.0)\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "good_model.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "good_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "good_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.1795 - acc: 0.3468 - val_loss: 2.0120 - val_acc: 0.4880\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 1.7823 - acc: 0.5444 - val_loss: 1.5655 - val_acc: 0.5504\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 1.4075 - acc: 0.5971 - val_loss: 1.2855 - val_acc: 0.6192\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 1.1934 - acc: 0.6392 - val_loss: 1.1253 - val_acc: 0.6536\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 1.0607 - acc: 0.6681 - val_loss: 1.0180 - val_acc: 0.6775\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.9655 - acc: 0.6916 - val_loss: 0.9355 - val_acc: 0.6978\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.8900 - acc: 0.7106 - val_loss: 0.8699 - val_acc: 0.7048\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.8282 - acc: 0.7254 - val_loss: 0.8153 - val_acc: 0.7194\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.7778 - acc: 0.7359 - val_loss: 0.7704 - val_acc: 0.7327\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.7375 - acc: 0.7428 - val_loss: 0.7357 - val_acc: 0.7394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11cef2048>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Stretch Challenge: Tune Your Hyperparameters!</h2></center>\n",
    "\n",
    "Congratulations--you've built your own working MultiLayer Perceptron using Keras! For those that are looking for an additional challenge, try tuning your hyperparameters and comparing your results! Keep track of how each does, and see if you can get a feel for what makes the model perform better, and what makes it perform worse.  \n",
    "\n",
    "Consider playing around with the following hyperparamters\n",
    "\n",
    "1. Activation Functions--which gives the best results?\n",
    "1. Depth--how does additional additional layer(s) change the performance of the model? Is there a point of **_diminishing returns_**, where the performance doesn't really increase but the runtime does?\n",
    "1. Layer width--how does changing the number of neurons in a layer affect the performance of the model?\n",
    "1. Optimizer--try other [optimizers available in Keras](https://keras.io/optimizers/)--for instance, `\"adam\"` or `\"rmsprop\"`.\n",
    "\n",
    "<h1>Answer:</h1>\n",
    "<p>The first method implemented. The more layers added, the harder it get to classify the data correctly.</p>\n",
    "\n",
    "Keep track of your results, and be scientific in your experimentation! Make small changes one at a time, and keep track of what happens each time.  If you make too many changes at once, you won't know how to **_assign blame!_** (See what I did there? Thats a backprop joke.  Admit it.  That was clever.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
