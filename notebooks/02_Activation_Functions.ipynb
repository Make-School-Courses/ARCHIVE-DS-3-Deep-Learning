{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you run this cell!\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Understanding Activation Functions</h1></center>\n",
    "\n",
    "**Goal:** Understand the different activation functions and their common uses. \n",
    "\n",
    "In this tutorial, you will examine the different activation functions common in Deep Learning, and gain some intuition for how and when to use each.  \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center><h3>Part 1: What is a Neuron Computing?</h3></center>\n",
    "\n",
    "Recall the architecture of a typical neural network.  The network is is made out of layers of neurons.  Each neuron in layer **_L_** takes in all the outputs from the previous layer (**_L - 1_**), computes an ouput, and passes that along to every neuron in the next layer (**_L + 1_**). Recall that when we talk about the layers of a neural network, we typically aren't talking about the _input layer_, only the **_hidden layer(s)_** and **_output layer_**.  Take a look at the following diagram for a refresher.  Each circle represents a neuron, and each line represents a weight.  \n",
    "\n",
    "\n",
    "<img src='http://www.astroml.org/_images/fig_neural_network_1.png' height=50% width=50%>\n",
    "\n",
    "But what happens _inside_ each neuron?  Each neuron computes their **_Z_** value, and feeds it through the an **_Activation Function._**  Let's take a look at at a diagram and see how we can gain some intuition for this process.\n",
    "<br>\n",
    "<br>\n",
    "<img src='http://3.bp.blogspot.com/-7RWgohC4pYE/VhtQ8IELsLI/AAAAAAAAA6I/_XFhMbjpcCY/s1600/Simple%2BNeural%2BNetwork.png' height=40% width=40%>\n",
    "\n",
    "Don't let all the mathematical notation in this diagram scare you, it's actually pretty simple! Let's work through it piece by piece before we compute some examples by hand.  \n",
    "\n",
    "\n",
    "\n",
    "Another way to represent the equation shown in the large neuron is **Z = w<sup>T</sup>x + b**.  **Z** is then fed through an **_Activation Function_**, to compute the output that will be propagated to the next layer.  \n",
    "\n",
    "Let's define each variable in this equation to make it easier to understand:\n",
    "\n",
    "**_w:_** A vector containing every weight value from every neuron from the previous layer. These weights start off as random values, but will shift towards optimal values during the training phase through backpropagation.  \n",
    "<br>\n",
    "**_<sup>T</sup>:_** Mathematical notation for \"_Transpose_\", which means \"flip the matrix over its diagonal\". In this case, it takes our 1-dimensional weight vector and rotates it so that we can easily compute the dot product of our weight vector and X, our inputs.  For example, a 4 x 1 vector would become a 1 x 4 vector.  \n",
    "<br>\n",
    "**_x:_** A vector containing all the outputs from the previous layer. The positions of each value in this vector match up with the position of the corresponding weight in the weight vector.  For instance, _w<sub>1</sub>_ and _X<sub>1</sub>_ are the values for the weight and output value from the 1st neuron of the previous layer.  \n",
    "\n",
    "**_b:_** A bias added to the equation to shift the decision boundary away from the origin. Recall that this value is set to a random value at initialization, and each neuron will learn the best bias along with the weights through backpropagation during the training phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center><h3>Test Your Understanding</h3></center>\n",
    "\n",
    "Try testing your understanding by manually computing z for the following neurons.  \n",
    "\n",
    "**_Hint:_** No need to do this manually--we need to compute the sum of inputs multiplied by their corresponding weights, plus the bias value.  This is the same as computing the **_dot product_** of w and x, and then adding b.  To compute the dot product of two vectors, just use numpy's [numpy.dot function](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.dot.html). An example is provided as a comment below.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1:  6.82\n",
      "z2:  [-4.26 -7.6  -2.8  -2.9  -5.6  -4.26]\n",
      "z3:  0.18367749528\n"
     ]
    }
   ],
   "source": [
    "# Problem 1: \n",
    "w1 = [-0.4, 0.82, 1.0, 0.5, 0.23, -0.75]\n",
    "x1 = [2, 3, 3, 7, 1, 4]\n",
    "b1 = 1.43\n",
    "\n",
    "#(SAMPLE SOLUTION: PROBLEM 1)\n",
    "z1 = np.dot(w1, x1)\n",
    "z1 += b1\n",
    "print(\"z1:  {}\".format(z1))\n",
    "\n",
    "#Problem 2:\n",
    "w2 = [-0.43, -2.1, 0.3, 0.25, -1.1, -0.43]\n",
    "x2 = [0.22, 0.34, 1.2, 0.00038, .63, -0.22]\n",
    "b2 = -3.4\n",
    "\n",
    "z2 = np.dot(w2, 2)\n",
    "z2 += b2\n",
    "print(\"z2:  {}\".format(z2))\n",
    "\n",
    "#Problem 3:\n",
    "w3 = [-0.00014, -.31105, 0.3, 0.000256, -.1145, -0.0000009]\n",
    "x3 = [0.22, 0.34, 1.2, 0.00038, .63, -0.22]\n",
    "b3 = 0.0016\n",
    "\n",
    "z3 = np.dot(w3, x3)\n",
    "z3 += b3\n",
    "print(\"z3:  {}\".format(z3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<br>\n",
    "z1: 6.819999999999999\n",
    "<br>\n",
    "z2:-4.446905\n",
    "<br>\n",
    "z3 0.18367749528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> What is an Activation Function?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neuron has now taken all weights and inputs, reduced them down to a single value, and added our bias.  What do we do with this number now?\n",
    "\n",
    "We feed through an **_Activation Function_**, which will determine what message the neuron passes on to the next layer.  In order to understand this, it's helpful to remember the inspiration behind neural networks--neurons in a brain!\n",
    "\n",
    "<img src='https://i.giphy.com/media/FLw63FKxHyVHO/giphy.webp'>\n",
    "\n",
    "A neural network is a _symbolic representation_ of the structure and function of neurons in a brain.  This means that although the details may be different, the general purpose and structure are the same--neural networks are a symbolic representation of neurons in a brain in the same way that airplanes are a symbolic representation of birds.\n",
    "\n",
    "In a brain, a neuron takes in the inputs from other neurons (through the dendrites on the left side of the picture) , and if the combination of those inputs are enough, it \"fires\" by passing an electrical charge down through the axon to any other connected neurons.  A neuron either fires or doesnt fire--on or off.  There is no \"in between\" state.  In this way, the neurons of our neural network are different--depending on the **_activation function_** used, our neurons do not have this limitation.  \n",
    "\n",
    "In simpler versions such as Single-Layer Perceptrons, the activation function is a simple step function. If the value of Z is greater than 1, the perceptron fires, passing a value of 1.  Otherwise, it does not fire, passing along a value of 0.  This may work for perceptrons, but for Deep Neural Networks, it leaves a lot to be desired. We'll skip over the step activation function and move right onto the activation functions most commonly used in Deep Learning: **_Sigmoid, Tanh, reLU,_** and **_Softmax_**\n",
    "\n",
    "From here on out, we'll code up examples of every activation function from scratch to get an intuition for how each one works.  We'll also explore the best use cases for each by looking at an examples of each in real neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing our training and testing sets, don't worry if you don't \n",
    "# Understand this part--we'll cover this in depth in other tutorials!\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sigmoid</h2></center>\n",
    "\n",
    "The **_sigmoid_** maps all possible real number inputs from negative infinity to infinity to a value between 0 and 1.  The function is called a sigmoid because when graphed, it resembles an 'S'.  If you're familiar with logistic regression, you'll likely recognize this function.    \n",
    "\n",
    "<center><img src='https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/I/m/Logistic-curve.svg.png'></center>\n",
    "\n",
    "The equation for the sigmoid function is: \n",
    "\n",
    "<center><img src='http://file.scirp.org/Html/htmlimages/10-9402081x/34ca7298-136f-4b25-a192-1e7f24401b53.png'></center>\n",
    "\n",
    "In this case, **x** will be equal to Z, which is equal to the dot product of our weights and inputs, plus the bias value. \n",
    "\n",
    "**_Challenge:_** Complete the function below so that it returns the logistic sigmoid of the input.  (Hint: make use of numpy's [numpy.exp() function!](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.exp.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1: None    Expected output: 0.817574476\n",
      "test 2: None    Expected output: 0.450166002\n",
      "test 3: None    Expected output: 0.998770601\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    s = None\n",
    "    return s\n",
    "\n",
    "print('test 1: {}    Expected output: {}'.format(sigmoid(1.5),  0.817574476))\n",
    "print('test 2: {}    Expected output: {}'.format(sigmoid(-0.2), 0.450166002))\n",
    "print('test 3: {}    Expected output: {}'.format(sigmoid(6.7),  0.998770601))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Use Cases: Sigmoid</h3></center>\n",
    "\n",
    "A sigmoid function can be used as a common activation function for any typical layer in a neural network.  This is a simple but effective way to introduce non-linearity into our model.  Although there are other activation functions that have become more popular in Deep Learning as of late, it is still very common to see sigmoids as the activation function for the output layer in binary classifiers.  This makes intuitive sense, since the _output of a sigmoid can be easily interpreted as a percentage value between 0 and 100%_!\n",
    "\n",
    "<center><h3>Example Use: Sigmoid</h3></center>\n",
    "\n",
    "In this example, we'll build an image classifier using the popular MNIST data set.  During this step, we'll use sigmoid activation functions in every layer except for the last one, which will be a softmax (there are 10 possible classes, so a sigmoid activation function would not make sense here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 2.2934 - acc: 0.1372 - val_loss: 2.2595 - val_acc: 0.1492\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 2.2378 - acc: 0.2900 - val_loss: 2.2106 - val_acc: 0.2920\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 2.1822 - acc: 0.4041 - val_loss: 2.1437 - val_acc: 0.4873\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 2.1028 - acc: 0.4948 - val_loss: 2.0470 - val_acc: 0.4927\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 1.9891 - acc: 0.5232 - val_loss: 1.9117 - val_acc: 0.5383\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 1.8411 - acc: 0.5577 - val_loss: 1.7496 - val_acc: 0.5694\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 1.6774 - acc: 0.5916 - val_loss: 1.5856 - val_acc: 0.5971\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 1.5221 - acc: 0.6244 - val_loss: 1.4387 - val_acc: 0.6405\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 1.3851 - acc: 0.6568 - val_loss: 1.3100 - val_acc: 0.6740\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 1.2658 - acc: 0.6820 - val_loss: 1.1984 - val_acc: 0.7013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1326bbcf8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Takeaways: Sigmoid Activation Functions</h3></center>\n",
    "\n",
    "After 10 epochs of training on the MNIST data set, we were able to obtain an **accuracy of 70%** on our validation set--not bad!  There's still room for improvement here.  This shows us that sigmoid activation functions do a passable job at capturing nonlinearity presented in the model.  Let's hold on to that 70% number and see how it stacks up against other activation functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Tanh</h2></center>\n",
    "\n",
    "Tanh (pronounched \"tanch\") stands for  _Hyperbolic Tangent_.  When plotted, the tanh function looks very similar to the sigmoid activation function.  This is because tanh is just a rescaled logistic sigmoid function! The main difference between the two is whereas the sigmoid activation function is bounded at 0 and 1, the tanh activation function is bounded at -1 and 1.  Let's take a look at a graph of the tanh function:\n",
    "\n",
    "<img src='http://mathworld.wolfram.com/images/interactive/TanhReal.gif'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Take a look at the plot below to see how tanh compares to the sigmoid function. The red line is a plot of the <font color='red'>tanh function</font>, and the blue line is the <font color='blue'>sigmoid function</font>. \n",
    "\n",
    "<img src='http://brenocon.com/blog/wp-content/uploads/2013/10/Screen-Shot-2013-10-31-at-4.32.04-PM.png' height=50% width=50%>\n",
    "\n",
    "\n",
    "We won't worry about writing a function to manually calculate the tanh function, but for reference, here it is:\n",
    "\n",
    "<center><img src='img/tanh_equation.png'></center>\n",
    "\n",
    "<center><h3>Use Cases: Tanh</h3></center>\n",
    "The tanh function is generally superior to the sigmoid function for training neural networks.  There are two main reasons for this:\n",
    "\n",
    "1.  Stronger gradients, which allow for more efficient back propagation.  \n",
    "2.  (Normalized) Data will be centered around 0 with a tanh activation function.  This is important, since the output of this layer will be the input of the next layer.  This helps prevent internal covariate shift--where each successive layer starts slightly biasing in a continuing direction (for intuition on this, take a look at what 0 outputs on a sigmoid function, and then use that as your input.  How did it change?)\n",
    "\n",
    "If you're interested in really digging into the reasons why tanh is a better choice than a sigmoid function, check out Yann LeCunn's excellent paper on <a href='http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf'>Efficient Backprop</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 1.1402 - acc: 0.7311 - val_loss: 0.6608 - val_acc: 0.8552\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.5636 - acc: 0.8658 - val_loss: 0.4646 - val_acc: 0.8855\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.4417 - acc: 0.8852 - val_loss: 0.3919 - val_acc: 0.8972\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3864 - acc: 0.8954 - val_loss: 0.3515 - val_acc: 0.9043\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3537 - acc: 0.9026 - val_loss: 0.3259 - val_acc: 0.9107\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.3310 - acc: 0.9073 - val_loss: 0.3094 - val_acc: 0.9148\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.3138 - acc: 0.9114 - val_loss: 0.2947 - val_acc: 0.9212\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2999 - acc: 0.9149 - val_loss: 0.2825 - val_acc: 0.9213\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.2881 - acc: 0.9184 - val_loss: 0.2736 - val_acc: 0.9243\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2778 - acc: 0.9209 - val_loss: 0.2654 - val_acc: 0.9258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14175fcc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_model = Sequential()\n",
    "tanh_model.add(Dense((64), activation='tanh', input_shape=(784,)))\n",
    "tanh_model.add(Dense((64), activation='tanh'))\n",
    "tanh_model.add(Dense((10), activation='softmax'))\n",
    "tanh_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "tanh_model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>reLU/Leaky reLU</h2></center>\n",
    "\n",
    "ReLU is short for **_Rectified Linear Unit_**.  For tasks such as computer vision, the discovery of the reLU activation function was revolutionary. Although it's highly effective, it's also easy to understand--reLU works by returning 0 for any for any z-values less than 0, and the value for anything greater than or equal to 0.  When graphed, reLU looks like this:\n",
    "\n",
    "<center><img src='img/relu.png'></center>\n",
    "\n",
    "There's also another popular variant of this activation function, called **_Leaky ReLU_**.  For inputs greater than or equal to zero, leaky reLU and classic reLU act the same way.  However, for negative inputs, leaky relu instead multiplies these inputs by a very small scalar value (0.1 seems to be most common) and outputs the result.  In this way, leaky reLU lets some of the signal from negative inputs \"leak\" through.  Some researchers report that using _leaky ReLU_ helps them avoid a the **_vanishing/exploding gradient problem_**--we won't dive into that problem in this assignment, but if you'd like to read more about it, check out the [wikipedia article on vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)!\n",
    "\n",
    "<center><img src='img/leaky_relu.png'></center>\n",
    "\n",
    "\n",
    "Let's build a model that makes use of the reLU activation function and see how it compares to Sigmoid and Tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 1.3803 - acc: 0.6478 - val_loss: 0.6532 - val_acc: 0.8470\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.5310 - acc: 0.8626 - val_loss: 0.4214 - val_acc: 0.8873\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.4043 - acc: 0.8891 - val_loss: 0.3553 - val_acc: 0.9005\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.3549 - acc: 0.9003 - val_loss: 0.3193 - val_acc: 0.9085\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.3254 - acc: 0.9078 - val_loss: 0.2987 - val_acc: 0.9151\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.3045 - acc: 0.9143 - val_loss: 0.2829 - val_acc: 0.9210\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2884 - acc: 0.9186 - val_loss: 0.2696 - val_acc: 0.9221\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.2749 - acc: 0.9213 - val_loss: 0.2600 - val_acc: 0.9251\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2635 - acc: 0.9254 - val_loss: 0.2494 - val_acc: 0.9283\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2534 - acc: 0.9284 - val_loss: 0.2405 - val_acc: 0.9304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x141c06278>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_model = Sequential()\n",
    "relu_model.add(Dense((64), activation='relu', input_shape=(784,)))\n",
    "relu_model.add(Dense((64), activation='relu'))\n",
    "relu_model.add(Dense((10), activation='softmax'))\n",
    "relu_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "relu_model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Takeaways: ReLU</h2></center>\n",
    "\n",
    "As we can see from our example above, reLU is a really great choice as an activation function.  This is especially true for problems, such as MNIST. When we cover Convolutional Neural Networks later in the course, we'll see that reLU plays a pivotal role.  \n",
    "\n",
    "<center><h2>SoftMax</h2></center>\n",
    "\n",
    "The final activation function we'll look at in this lesson is a special one--the **_SoftMax_** activation function.  This function is special in that it is primarily only ever used for the output layer in a neural network.  Much like the sigmoid activation function, SoftMax outputs probabilities between 0 and 1.  Recall that we use sigmoid for _binary classification_ problems--when used as the activation function for the output layer, the float value between 0 and 1 corresponds to the percentage chance that the input is or isn't the thing you're trying to detect.  But what about problems such as the iris dataset, where we have more than two classes? This is where SoftMax comes in.  SoftMax is used as the activation function for output layers in **_multiclass classification problems_**.  To accomplish this, SoftMax outputs a _vector of probabilities_, where each number in the vector corresponds to the how likely it is that the input belongs to each corresponding class.  For instance, let's say we assigned an integer label to each flower type in the Iris dataset, with `setosa = 0`, `versicolor = 1`, and `virginica = 2`.  If, for a given example, the SoftMax output a vector with the values `[0.07, 0.89, 0.04]`, then we could interpret that as the neural network predicting a 7% chance that the flower is a setosa, 89% chance the flower is a versicolor, and 4% chance that the flower is a virginica.  \n",
    "\n",
    "**_Note:_**  You may have noticed that in the example above, the 3 probabilities within the vector output by the SoftMax add up to exactly 1.  This is no accident.  As long as we make sure that the output layer contains the same number of neurons as the number of total classes we are trying to predict, the vector will contain a probability for every possible class, and the sum of all the probabilities will aways be exactly 1.  \n",
    "\n",
    "<center><img src='img/softmax_activation.png' height=80% width=80%></center>\n",
    "\n",
    "Take a look at the example image below.  In the next cell, we'll generate a prediction for the image, and then take a look at the output from the SoftMax output below.  We'll see an output vector of 10 probabilities, and if our neural network has learned to predict the images correctly, then we should also expect the value at index 5 to have the highest value.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1416f4dd8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123c6db00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "(X_pred, _), _= mnist.load_data()\n",
    "reshape_pred = X_pred.reshape(60000, 784).astype(\"float32\")\n",
    "reshape_pred /= 255.\n",
    "plt.imshow(X_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood for 0: 0.261%\n",
      "Likelihood for 1: 0.002%\n",
      "Likelihood for 2: 0.342%\n",
      "Likelihood for 3: 14.65%\n",
      "Likelihood for 4: 0.000%\n",
      "Likelihood for 5: 84.57%\n",
      "Likelihood for 6: 0.001%\n",
      "Likelihood for 7: 0.037%\n",
      "Likelihood for 8: 0.084%\n",
      "Likelihood for 9: 0.038%\n",
      "Prediction: 5\n",
      "Certainty: 84.58%\n"
     ]
    }
   ],
   "source": [
    "preds = relu_model.predict(reshape_pred)\n",
    "for ind, val in enumerate(preds[0]):\n",
    "    print(\"Likelihood for {}: {}%\".format(ind, str(val * 100)[:5]))\n",
    "print(\"Prediction: {}\".format(np.argmax(preds[0])))\n",
    "print(\"Certainty: {:.2%}\".format(np.max(preds[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Takeaways: SoftMax</h2></center>\n",
    "\n",
    "Great! Our network works, and we can easily interpret the results from the SoftMax. Our classifier was 85.57% sure that the image in question is a 5.  Interestingly enough, the network was also 13% sure that it could also be a 3.  When we look at the image, it's easy to see how this sloppy 5 could possibly also be seen as a 3.  \n",
    "\n",
    "<center><h1>Bringing It All Together</h1></center>\n",
    "\n",
    "Let's recap what we've learned so far:\n",
    "\n",
    "<strong>1. For classification, every neuron in a neural network needs an **_Activation Function_**.</strong>\n",
    "<br>\n",
    "**2. The neuron calculates the weighted sum of its inputs, and then passes this sum to the activation function.**\n",
    "<br>\n",
    "**3. In a Feed-Forward Neural Network, the output of the activation function is the input for neurons in the next layer.  **\n",
    "\n",
    "**_Challenge:_** Below, you'll find a list of the 4 activation functions explained in this notebook.  Below each activation function, provide a brief explanation of how it works.  Be sure to include what each is used for!\n",
    "\n",
    "\n",
    "<center><h3>Sigmoid</h3></center>\n",
    "**_Description:_**\n",
    "<p>A sigmoid function can be used as a common activation function for any typical layer in a neural network. This is a simple but effective way to introduce non-linearity into our model.</p>\n",
    "\n",
    "<center><h3>Tanh</h3></center>\n",
    "**_Description:_**\n",
    "<p>The tanh function is generally superior to the sigmoid function for training neural networks. There are two main reasons for this: Stronger gradients, which allow for more efficient back propagation.\n",
    "(Normalized) Data will be centered around 0 with a tanh activation function. This is important, since the output of this layer will be the input of the next layer. This helps prevent internal covariate shift--where each successive layer starts slightly biasing in a continuing direction (for intuition on this, take a look at what 0 outputs on a sigmoid function, and then use that as your input.</p>\n",
    "\n",
    "<center><h3>reLU</h3></center>\n",
    "**_Description:_**\n",
    "<p>ReLU(Rectified Linear Unit) For tasks such as computer vision, the discovery of the reLU activation function was revolutionary. Although it's highly effective, it's also easy to understand--reLU works by returning 0 for any for any z-values less than 0, and the value for anything greater than or equal to 0. </p>\n",
    "<center><h3>SoftMax</h3></center>\n",
    "**_Description:_**\n",
    "<p>This function is special in that it is primarily only ever used for the output layer in a neural network. Much like the sigmoid activation function, SoftMax outputs probabilities between 0 and 1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
